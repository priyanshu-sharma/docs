Project Narratives

1.Give a description of your projects:
a. Why were you brought there initially?
b. (Mobile) What the current state of app and production environment upon your arrival?
c. (Mobile) What was the app and what does it do?
d. (Mobile) Who was the end user of the app?
e. (Mobile) What did you specifically implement feature wise in the app and how?
f. What percent of time did you lead and what percent of time did you code?
g. What challenges did you face in the production environment and in development and how did you address them? 
h. What was the size of the team and break down of roles?
i. How did you manage the day-to-day production environment, task management and tool, documentation standards? 
j. What was the testing environment like? What tools and practices were used to manage sound code?
k. What weekly standing meetings were present, who was present, and what was discussed? How did you run the scrum meeting?

2. define 2-3 major concepts of your actual, tangible contribution to the app and production environment  (assessing current state, level setting expectations, redefining processes and technologies, and addressing any existing bugs and perfecting/updating the existing code before moving onto new features)
IF - From scratch
a. The tools used to accomplish the requirements and making decisions related to this (data storage, data access, architecture, design, testing, driving the production environment)
b. Architecture awareness – Delve into your understanding and decision-making process around system architecture. Highlight any trade-offs or considerations you had to make and why
c. Layouts awareness - Discuss your approach to designing user interfaces or system layouts, ensuring that the end-user experience is seamless.  
d. Gathering requirements ability and having those meetings with stakeholders
e. What did you specifically implement feature wise and how
f. SDLC - share how you adhered to the Software Development Life Cycle, ensuring that all phases were effectively managed and executed
g. QA - Discuss any testing strategies employed, challenges faced, and how you ensured that the final product was free of critical bugs

IF - From Existing production environment
a. Quickly learning and combing through code base and other existing technologies used in production
b. Check the current reviews about the product and app based on end-user experience and work to improvement those
c. Looking through the crash logs and looking through any existing bugs in the back log to address
d. Looking through the entirety of the back log of tasks (if any) to come up with a plan based on priority to address throughout the SDLC (of course, keeping in mind the MVP)
e. Re-defining the production environment practices and coding style to best practice (OOP, for example) 

3. An example of technical failure in a production environment, why, how did you rectify, and what is your key learning from this to take forward into your future development practice?

Now - Tell us about this project.  (Why you were hired, status at the time, How did you work with your team to accomplish goal, skills/tech you used, issue resolved, what are the deliverables)

Can you answer all the following questions based on my past job experience


Project Narratives

1. The tools used to accomplish the requirements and making decisions related to this (data storage, data access, architecture, design, testing, driving the production environment)
2. Architecture awareness – Delve into your understanding and decision-making process around system architecture. Highlight any trade-offs or considerations you had to make and why
3. Layouts awareness - Discuss your approach to designing user interfaces or system layouts, ensuring that the end-user experience is seamless.  
4. Gathering requirements ability and having those meetings with stakeholders
5. What did you specifically implement feature wise and how
6. SDLC - share how you adhered to the Software Development Life Cycle, ensuring that all phases were effectively managed and executed
7. QA - Discuss any testing strategies employed, challenges faced, and how you ensured that the final product was free of critical bugs
8. Now - Tell us about this project.  (Why you were hired, status at the time, How did you work with your team to accomplish goal, skills/tech you used, issue resolved, what are the deliverables)

Job experience

Aug 2021 – Sep 2023 · Sr. Data Engineer · Centene Corporation, St. Louis, MO


    • Leveraged the Cloud Storage Transfer Service for swift and secure data movement between on-premises systems and GCP at Centene.
    • Engineered and sustained large-scale data processing and analysis pipelines with Apache Spark and Python on Google Cloud Platform (GCP).
    • Transferred data to optimal storage solutions like Google Cloud Storage (GCS), Bigtable, or BigQuery tailored to analytical needs.
    • Employed Google Dataprep to ensure clean and prepared data during migration, with monitoring by Cloud Monitoring.
    • Managed data migration using Cloud Composer for a smooth and controlled process.
    • Crafted and executed efficient data models and schema designs in BigQuery for enhanced querying and storage.
    • Integrate Hive with other Big Data tools and frameworks such as Apache Pig, Spark, and Kafka to build end-to-end data processing and analytics solutions. 
    • Defined a scalable and comprehensive data architecture that integrates Snowflake, Oracle, GCP services, and other essential components.
    • Applied Vertex AI Pipelines (formerly Kubeflow Pipelines) to orchestrate machine learning workflows on GCP.
    • Set up and configured BigQuery datasets, tables, and views for effective storage and management of transformed data.
    • Established data quality checks and validation rules to ensure data accuracy and reliability in BigQuery.
    • Linked BigQuery with other GCP services for various purposes, including data visualization (Data Studio), AI/ML analysis, and long-term data archiving (Cloud Storage).
    • Employed Google Cloud Storage for data ingestion and Pub/Sub for event-driven data processing.
    • Design and implement data models and schemas in Hive to represent complex business requirements, ensuring data integrity and optimal performance for analytics queries. 
    • Developed ETL pipelines using methods like CDC (Change Data Capture) or scheduled batch processing to extract data from Oracle databases and transfer it to BigQuery.
    • Implemented Cloud Billing reports and recommendations to enhance and optimize GCP resource usage for cost-efficiency.
    • Implement ETL pipelines using Apache Airflow to extract data from various sources, transform it into a usable format, and load it into data warehouses or data lakes for analytics and reporting.
    • Integrate DBT with data warehouse solutions such as Snowflake, BigQuery, Redshift, or Azure Synapse, ensuring seamless data transformation and modeling within the data ecosystem.
    • Configure DBT to interact with data warehouses, including setting up connections, managing schema changes, and handling data versioning. 
    • Develop custom operators and sensors in Airflow to handle specific data transformation and loading tasks, ensuring seamless integration with Big Data technologies such as Hadoop, Spark, and Kafka. 
    • Formulated data models and schema designs for Snowflake data warehouses to support intricate analytical queries and reporting.
    • Write Scala-based ETL pipelines to extract, transform, and load (ETL) data into Big Data platforms, optimizing performance for handling massive datasets. 
    • Managed diverse data sources (structured, semi-structured, unstructured) to design data integration solutions on GCP.
    • Executed real-time data processing using Spark, GCP Cloud Composer, and Google Dataflow with PySpark ETL jobs for effective analysis.
    • Developed data ingestion pipelines (Snowflake staging) from various sources and data formats to enable real-time analytics.
    • Connected data pipelines with various data visualization and BI tools like Tableau and Looker for generating dashboards and reports.
    • Guided junior data engineers, offering mentorship on ETL best practices, Snowflake, Snowpipes, and JSON.
    • Implemented infrastructure provisioning using Terraform for consistent and repeatable environments across different project stages.
    • Utilized Kubernetes to manage the deployment, scaling, and lifecycle of Docker containers.
    • Enhanced ETL and batch processing jobs for performance, scalability, and reliability using Spark, YARN, and GCP DataProc.
    • Administered and optimized GCP resources (VMs, storage, network) for cost-effectiveness and performance.
    • Configured Cloud Identity & Access Management (IAM) roles to ensure least privilege access to GCP resources at Centene.
    • Integrate Scala-based applications with Big Data ecosystems, including Apache Hadoop, Hive, HBase, Cassandra, and others, to enable efficient data storage and retrieval. 
    • Utilized Google Cloud Composer to build and deploy data pipelines as DAGs using Apache Airflow.
    • Built a machine learning pipeline using Apache Spark and scikit-learn for training and deploying predictive models.
    
Can you answer all the project Narratives questions based on my past job experience


Truist Financial provides several API services, mainly focused on enabling integration with its financial products and services. These APIs are designed to facilitate various banking and financial functionalities for developers and businesses. Some of the key API services offered by Truist include:

1. **Account and Transaction APIs**: These APIs allow access to account information and transaction history. They support functions like retrieving account balances, transaction details, and other related financial data.

2. **Payment Initiation APIs**: These APIs enable the initiation of payments directly from a user’s account. This includes ACH (Automated Clearing House) transactions, wire transfers, and other payment methods.

3. **Authentication and Identity APIs**: These APIs are used for verifying the identity of users and managing authentication flows, such as Single Sign-On (SSO) and OAuth integrations.

4. **Open Banking and PSD2 APIs**: Truist also supports open banking APIs compliant with PSD2 (Payment Services Directive 2) regulations, enabling third-party providers to access customer financial data securely with user consent.

5. **Developer Tools and Environments**: Truist offers a sandbox environment, API Explorer, and other developer tools like Postman collections and OpenAPI/Swagger specifications to help developers test and integrate APIs effectively.

For more details, you can explore Truist's [Developer Center](https://developer.truist.com/api/working-with-truist) and their API offerings on [API Tracker](https://apitracker.io/a/truist/apis)【6†source】【7†source】【8†source】【9†source】【10†source】.


Centene Corporation offers several API services designed to facilitate the integration of healthcare data and services for third-party developers. Some of the key APIs provided by Centene include:

Member Eligibility API: This API allows third-party applications to check the eligibility status of members for various healthcare services. It helps in verifying if a member is covered under a specific health plan.

Provider Search API: This API enables applications to search for healthcare providers within Centene's network. It provides details about providers, including their specialties, locations, and contact information.

Claims Submission API: This API allows healthcare providers and third-party applications to submit claims electronically. It supports the submission of claims data in a standardized format to streamline the claims processing workflow.

Authorization API: This API is used for managing prior authorization requests for healthcare services. It helps in determining whether a particular service or treatment requires prior approval and facilitates the submission of authorization requests.

Appointment Scheduling API: This API helps in managing appointment bookings for healthcare services. It provides functionalities for scheduling, rescheduling, or canceling appointments with healthcare providers.